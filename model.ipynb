{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# added\n",
    "from folder import KFolder\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from Augmentation import ToAugmantate\n",
    "from shuffle import shuffle_data\n",
    "import pickle\n",
    "from load_data import serialize_model, deserialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(hist):\n",
    "    train_epochs = list(range(1, len(hist['loss']) + 1))\n",
    "    valid_epochs = list(range(1, len(hist['val_loss']) + 1))\n",
    "    train_losses = hist['loss']\n",
    "    valid_losses = hist['val_loss']\n",
    "\n",
    "    train_min_loss = min(dict(zip(train_epochs, train_losses)).items(), key=lambda x: x[1])\n",
    "    valid_min_loss = min(dict(zip(valid_epochs, valid_losses)).items(), key=lambda x: x[1])\n",
    "\n",
    "    plt.plot(train_epochs, hist['loss'], linewidth=1, color='blue', label='Training Loss')\n",
    "    plt.plot(train_min_loss[0], train_min_loss[1], color='black', marker='o',\n",
    "             label=f'Training Best Epoch - {train_min_loss[0]}')\n",
    "    plt.plot(valid_epochs, hist['val_loss'], linewidth=1, color='red', label='Validation Loss')\n",
    "    plt.plot(valid_min_loss[0], valid_min_loss[1], color='black', marker='o',\n",
    "             label=f'Validation Best Epoch - {valid_min_loss[0]}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(label='Training & Validation Loss')\n",
    "    plt.xlabel(xlabel='Epochs')\n",
    "    plt.ylabel(ylabel='Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plot(hist):\n",
    "  epochs = list(range(1, len(hist['accuracy']) + 1))\n",
    "\n",
    "  plt.plot(epochs, hist['accuracy'], marker='o', linewidth=1, label='Training Accuracy', color='blue')\n",
    "  plt.plot(epochs, hist['val_accuracy'], marker='o', linewidth=1, label='Validation Accuracy', color='red')\n",
    "  plt.title(label='Training & Validation Accuracy')\n",
    "  plt.legend(loc='best')\n",
    "  plt.xlabel(xlabel='Epochs')\n",
    "  plt.ylabel(ylabel='Accuracy')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = keras.models.Sequential(layers=[\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), input_shape=(150, 150, 3),\n",
    "            padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dropout(rate=0.2),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# relu, softmax\n",
    "# relu -> gelu, softmax -> softplus,\n",
    "# gelu -> relu, softplus\n",
    "# relu -> gelu, softplus -> softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DON'T RUN EVERY TIME, ONLY IF U WANT TO SHUFFLE TRAIN <-> TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import sys\n",
    "# output_dir = 'splitted_dataset'\n",
    "# if os.path.exists(output_dir):\n",
    "#    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "# os.makedirs(output_dir)\n",
    "# import splitfolders\n",
    "# splitfolders.ratio('dataset', output=output_dir, seed=1337, ratio=(.9, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path: str, class_id: int) -> list:\n",
    "    content_list = os.listdir(path)\n",
    "    files_list = [os.path.join(path, content_thing) for content_thing in content_list if os.path.isfile(os.path.join(path, content_thing))]\n",
    "    imgs_list = []\n",
    "    labels_list = []\n",
    "    for file_path in files_list:\n",
    "        temp_img = Image.open(file_path)\n",
    "        numpy_image = np.asarray(temp_img.resize((150,150)))\n",
    "        imgs_list.append(numpy_image)\n",
    "        labels_list.append(class_id)\n",
    "    return imgs_list, labels_list\n",
    "\n",
    "# load train images\n",
    "main_path = \"splitted_dataset/\"\n",
    "cataract = main_path+\"train/cataract\"\n",
    "diabetic_retinopathy = main_path+\"train/diabetic_retinopathy\"\n",
    "glaucoma = main_path+\"train/glaucoma\"\n",
    "normal = main_path+\"train/normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cataract, labels_cataract = load_images(cataract, 1)\n",
    "images_diabetic_retinopathy, labels_retinopathy = load_images(diabetic_retinopathy, 2)\n",
    "images_glaucoma, labels_glaucoma = load_images(glaucoma, 3)\n",
    "images_normal, labels_normal = load_images(normal, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tables\n",
    "x = images_cataract+images_diabetic_retinopathy+images_glaucoma+images_normal\n",
    "y = labels_cataract+labels_retinopathy+labels_glaucoma+labels_normal\n",
    "x, y = shuffle_data(x, y)\n",
    "del images_cataract, images_diabetic_retinopathy, images_glaucoma, images_normal, labels_cataract,labels_retinopathy,labels_glaucoma,labels_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cataract, labels_cataract = load_images(main_path+\"val/cataract\", 1)\n",
    "images_diabetic_retinopathy, labels_retinopathy = load_images(main_path+\"val/diabetic_retinopathy\", 2)\n",
    "images_glaucoma, labels_glaucoma = load_images(main_path+\"val/glaucoma\", 3)\n",
    "images_normal, labels_normal = load_images(main_path+\"val/normal\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = images_cataract+images_diabetic_retinopathy+images_glaucoma+images_normal\n",
    "Y_test = labels_cataract+labels_retinopathy+labels_glaucoma+labels_normal\n",
    "X_test, Y_test = shuffle_data(X_test, Y_test)\n",
    "del images_cataract, images_diabetic_retinopathy, images_glaucoma, images_normal, labels_cataract,labels_retinopathy,labels_glaucoma,labels_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ToAugmantate()\n",
    "test_data_gen = ToAugmantate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight  = 'balanced', classes = np.unique(y), y=y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "# class_weights = generate_class_weights(train_data_gen)\n",
    "n_classes = len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolder = KFolder(10, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import tensorflow as tf\n",
    "# # from dataset import get_data\n",
    "# # from model import get_model\n",
    "# # import os\n",
    "# # import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # import segmentation_models as sm\n",
    "# # from folder import KFolder\n",
    "# for i, fold in enumerate(kfolder.folds):\n",
    "#     callbacks = [tf.keras.callbacks.ModelCheckpoint(f\"./best_model_fold_{i}.h5\", save_best_only=True, verbose=1)]\n",
    "#     # callbacks = [tf.keras.callbacks.ModelCheckpoint(\"./best_model.h5\", save_best_only=True, verbose=1)]\n",
    "#     model = get_model()\n",
    "#     model.compile(loss='mean_squared_logarithmic_error', optimizer='adam')\n",
    "\n",
    "#     X_train, y_train, X_valid, y_valid = fold\n",
    "\n",
    "#     model.fit(train_data_gen.flow(X_train, y=y_train, batch_size=8, subset='training'), validation_data=(train_data_gen.flow(X_valid, y_valid, batch_size=8, subset='validation')), batch_size=16, epochs=100, callbacks=callbacks, verbose=1)\n",
    "\n",
    "#     del model\n",
    "#     best_model = tf.keras.models.load_model(f\"./best_model_fold_{i}.h5\")\n",
    "\n",
    "#     evaluation = best_model.evaluate(X_test, Y_test)\n",
    "\n",
    "#     with open(f\"./results_fold_{i}.txt\",'w') as f:\n",
    "#         f.write(f\"test loss: {evaluation[0]}\\n\")\n",
    "#         f.write(f\"accuracy: {evaluation[1]}\\n\")\n",
    "#         f.write(f\"dice-score: {evaluation[2]}\\n\")\n",
    "#     print(100*'--')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above presented by Chat GPT, bc above was giving error I had no clue why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model_loss = float('inf')\n",
    "best_model_fold_loss = []\n",
    "best_model_accuracy = 0.0\n",
    "best_model_fold_accuracy = []\n",
    "\n",
    "histories = []\n",
    "\n",
    "for fold_idx, (x_train, y_train, x_valid, y_valid) in enumerate(kfolder.folds):\n",
    "    print(\"Fold:\", fold_idx + 1)\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0003)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_data_gen.flow(x_train, y_train, batch_size=32),\n",
    "                            epochs=35,\n",
    "                            validation_data=test_data_gen.flow(x_valid, y_valid),\n",
    "                            callbacks=[early_stopping, reduce_lr],\n",
    "                            verbose=1, \n",
    "                            class_weight=class_weights\n",
    "                        )\n",
    "\n",
    "    # Append loss and accuracy history for this fold\n",
    "    histories.append(history.history)\n",
    "    # Evaluate the model on validation data\n",
    "    loss, accuracy = model.evaluate(test_data_gen.flow(x_valid, y_valid))\n",
    "    print(\"Fold Loss:\", loss)\n",
    "    print(\"Fold Accuracy:\", accuracy)\n",
    "    \n",
    "\n",
    "    # Save the best model for this fold\n",
    "    if loss < best_model_loss:\n",
    "        best_model = model\n",
    "        best_history = history.history\n",
    "        best_model_loss = loss\n",
    "    best_model_fold_loss.append(loss)\n",
    "    best_model_fold_accuracy.append(accuracy)\n",
    "\n",
    "print(\"Accuracy and Loss plots saved as PNG files\")\n",
    "# Print the best model's loss and accuracy for each fold\n",
    "print(\"Best Model Loss for each fold:\", best_model_fold_loss)\n",
    "print(\"Best Model Accuracy for each fold:\", best_model_fold_accuracy)\n",
    "\n",
    "# Save the best overall model\n",
    "serialize_model(best_model,'best_model', best_history )\n",
    "best_model.save('best_model.h5')\n",
    "print(\"Best Model saved as 'best_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "# Convert loss_histories and accuracy_histories to numpy arrays\n",
    "histories = np.array(histories)\n",
    "current_funcs = \"relus_&_softmax\"\n",
    "save2_file_name = current_funcs\n",
    "for i in range(0, len(histories)):\n",
    "    loss_plot(histories[i])\n",
    "    accuracy_plot(histories[i])\n",
    "    serialize_model(model, file_name = f'{save2_file_name}_fold{i+1}', history_model=histories[i], def_path=f'./Model/{current_funcs}/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "14/14 [==============================] - 1s 82ms/step\n",
      "14/14 [==============================] - 1s 80ms/step - loss: 0.2126 - accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100,   4,   0,   4],\n",
       "       [  2,  95,   0,   7],\n",
       "       [  0,   1, 109,   0],\n",
       "       [ 10,   4,   0,  87]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save2_file_name = \"best_model\"\n",
    "model, history_model = deserialize_model(save2_file_name, def_path=\"\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "eval = model.evaluate(X_test, Y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loss_histories and accuracy_histories to numpy arrays\n",
    "loss_plot(history_model)\n",
    "accuracy_plot(history_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.43\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       108\n",
      "           1       0.91      0.91      0.91       104\n",
      "           2       1.00      0.99      1.00       110\n",
      "           3       0.89      0.86      0.87       101\n",
      "\n",
      "    accuracy                           0.92       423\n",
      "   macro avg       0.92      0.92      0.92       423\n",
      "weighted avg       0.92      0.92      0.92       423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# get index of max for each element in array *predictions*\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Assuming Y_test contains continuous target values and predictions are also continuous\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "class_report = classification_report(Y_test, predictions)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print('Classification Report:\\n', class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.9527186761229315, 0.9574468085106383, 0.9976359338061466, 0.9408983451536643}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy_each_class = { accuracy_score(np.array(Y_test) == i, np.array(predictions) == i) for i in range(0, 4)}\n",
    "\n",
    "print(accuracy_each_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
